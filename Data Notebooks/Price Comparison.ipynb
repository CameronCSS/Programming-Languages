{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for pulling search words from a google sheet instead of user input\n",
    "# this is in preperation to set up schedule running of this script to auto update a database.\n",
    "\n",
    "import gspread\n",
    "import time\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import snowflake.connector\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from gspread.exceptions import WorksheetNotFound\n",
    "\n",
    "\n",
    "# Define the scope\n",
    "scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']\n",
    "\n",
    "# Add your Service Account File\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_name(r\"PATH_TO_CREDENTIALS\", scope)\n",
    "\n",
    "# authorize your client\n",
    "client = gspread.authorize(creds)\n",
    "\n",
    "\n",
    "def get_search_word_from_sheet():\n",
    "\n",
    "    # Open the Google Spreadsheet by its url (make sure you have access to it)\n",
    "    sheet = client.open_by_url('URL_FOR_GOOGLE_SHEET').sheet1\n",
    "\n",
    "    # Get all the records of the data\n",
    "    data = sheet.get_all_records()\n",
    "\n",
    "    # # Get today's day of the week\n",
    "    # today = datetime.today().strftime('%A')\n",
    "\n",
    "    # # Checks for day of the week from google sheet\n",
    "    # for row in data:\n",
    "    #     # If the day in the 'day_of_week' column matches today's day, return the corresponding search word\n",
    "    #     if row['day_of_week'] == today:\n",
    "    #         return row['search_word']\n",
    "\n",
    "    # Get today's date if searching by Date instead of day of week\n",
    "    today = date.today()\n",
    "\n",
    "    # Checks for date instead of day of the week\n",
    "    for row in data:\n",
    "        # If the date in the 'date' column matches today's date, return the corresponding search word\n",
    "        date_from_sheet = datetime.strptime(row['date'], '%m/%d/%Y').date()\n",
    "        if date_from_sheet == today:\n",
    "            return row['search_word']\n",
    "\n",
    "    return None\n",
    "\n",
    "# Scraping rules for the two websites\n",
    "def scrape_website(url, word):\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "\n",
    "    if \"livingspaces.com\" in url:\n",
    "        input_field = driver.find_element(By.ID, 'search')\n",
    "        input_field.send_keys(word)\n",
    "        form = input_field.find_element(By.XPATH, './ancestor::form')\n",
    "        form.submit()\n",
    "    elif \"rcwilley.com\" in url:\n",
    "        input_field = driver.find_element(By.ID, 'searchBox')\n",
    "        input_field.send_keys(word)\n",
    "        submit_button = driver.find_element(By.ID, 'searchSubmit')\n",
    "        submit_button.click()\n",
    "\n",
    "    time.sleep(5)  \n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    prices = []\n",
    "\n",
    "    if \"livingspaces.com\" in url:\n",
    "        product_items = soup.find_all('div', class_='product-item-container')\n",
    "        for item in product_items:\n",
    "            name_element = item.find('span', class_='name')\n",
    "            price_element = item.find('span', class_='price')\n",
    "            rating_element = item.find('div', class_='ratings')\n",
    "            if name_element and price_element and rating_element:\n",
    "                name = name_element.text.strip()\n",
    "                price = price_element.text.strip()\n",
    "\n",
    "                rating = 0\n",
    "                rating_text = rating_element['aria-label']\n",
    "                if rating_text:\n",
    "                    rating = float(rating_text.split(' out of ')[0])\n",
    "                rating = int(rating) if isinstance(rating, float) and rating.is_integer() else rating \n",
    "                # tried about 40 ways to get int and float to carry over. Snowflake had issues with 'rating' values. Turns out it was my google sheet with empty values. this still has extra checks just incase.\n",
    "\n",
    "                prices.append({'name': name, 'price': float(price.replace('$', '').replace(',', '')), 'rating': rating})\n",
    "\n",
    "    elif \"rcwilley.com\" in url:\n",
    "        product_items = soup.find_all('div', class_='productContent')\n",
    "        for item in product_items:\n",
    "            name_element = item.find('div', class_='productName')\n",
    "            price_element = item.find('span', class_='price')\n",
    "            if name_element and price_element:\n",
    "                name = name_element.text.strip()\n",
    "                price = price_element.text.strip()\n",
    "                rating_element = item.find('div', class_='rating')\n",
    "\n",
    "                rating = 0\n",
    "                if rating_element:\n",
    "                    rating_span = rating_element.find('span', class_='sr-only')\n",
    "                    if rating_span:\n",
    "                        rating_text = rating_span.text.strip()\n",
    "                        rating = float(''.join([i for i in rating_text if i.isdigit() or i == '.']))\n",
    "                rating = int(rating) if isinstance(rating, float) and rating.is_integer() else rating \n",
    "                # tried about 40 ways to get int and float to carry over. Snowflake had issues with 'rating' values. Turns out it was my google sheet with empty values. this still has extra checks just incase.\n",
    "                prices.append({'name': name, 'price': float(price.replace('$', '').replace(',', '')), 'rating': rating})\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    return prices\n",
    "\n",
    "# Calculating averages of items scraped\n",
    "def calculate_average_value(values):\n",
    "    values_float = [value for value in values if value is not None and value != 0]\n",
    "    average = sum(values_float) / len(values_float) if values_float else 0\n",
    "    return round(average, 2) if average is not None else 0\n",
    "\n",
    "# Get both min and max prices from the item list\n",
    "def get_min_max(prices):\n",
    "    if not prices:\n",
    "        return None, None\n",
    "    min_price = min(prices, key=lambda x: x['price'])\n",
    "    max_price = max(prices, key=lambda x: x['price'])\n",
    "    return min_price, max_price\n",
    "\n",
    "# Double check the IDs and continue counting\n",
    "def get_max_id(sheet_name):\n",
    "    # authorize your client\n",
    "    client = gspread.authorize(creds)\n",
    "\n",
    "    # Open the Google Spreadsheet by its url (make sure you have access to it)\n",
    "    sheet = client.open_by_url('URL_FOR_GOOGLE_SHEET')\n",
    "\n",
    "    try:\n",
    "        ws = sheet.worksheet(sheet_name)\n",
    "        data = ws.get_all_values()  # get all values inside the specified worksheet\n",
    "        if data:\n",
    "            # first column has ID, get max value\n",
    "            ids = [int(row[0]) for row in data[1:] if row[0].isdigit()]  # ignore first row(header)\n",
    "            if ids:\n",
    "                return max(ids)\n",
    "    except WorksheetNotFound:\n",
    "        pass\n",
    "    return 0\n",
    "\n",
    "# Function to add searchID to each search so we can better track our info over time.\n",
    "def get_search_id():\n",
    "    search_id = 0\n",
    "    sheets = ['item_prices', 'compared_prices']\n",
    "\n",
    "    # Open the Google Spreadsheet by its url\n",
    "    sheet = client.open_by_url('URL_FOR_GOOGLE_SHEET')\n",
    "\n",
    "    for sheet_name in sheets:\n",
    "        try:\n",
    "            ws = sheet.worksheet(sheet_name)\n",
    "            data = ws.get_all_values()  # get all values inside the specified worksheet\n",
    "            if data:\n",
    "                # first column has search_id, get max value\n",
    "                search_ids = [int(row[1]) for row in data[1:] if row[1].isdigit()]  # ignore first row(header)\n",
    "                if search_ids:\n",
    "                    search_id = max(max(search_ids), search_id)\n",
    "        except WorksheetNotFound:\n",
    "            pass\n",
    "    return search_id + 1\n",
    "\n",
    "\n",
    "# Final function that prints the comparisons, and adds them to the excel file\n",
    "def compare_prices(word, item_prices_max_id, compared_prices_max_id):\n",
    "\n",
    "    website1_url = 'https://www.livingspaces.com/'\n",
    "    website2_url = 'https://www.rcwilley.com/'\n",
    "\n",
    "    prices_website1 = scrape_website(website1_url, word)\n",
    "    prices_website2 = scrape_website(website2_url, word)\n",
    "\n",
    "    # Calculate average prices\n",
    "    average_website1 = calculate_average_value([price['price'] for price in prices_website1])\n",
    "    average_website2 = calculate_average_value([price['price'] for price in prices_website2])\n",
    "\n",
    "    # Calculate average ratings\n",
    "    average_rating_website1 = calculate_average_value([float(price['rating']) for price in prices_website1 if price['rating'] is not None])\n",
    "    average_rating_website2 = calculate_average_value([float(price['rating']) for price in prices_website2 if price['rating'] is not None])\n",
    "\n",
    "\n",
    "    website1_name = website1_url.replace('https://www.', '').replace('.com/', '').capitalize()\n",
    "    website2_name = website2_url.replace('https://www.', '').replace('.com/', '').capitalize()\n",
    "\n",
    "    print(f\"Item: {search_word}\")\n",
    "    print()\n",
    "\n",
    "    print(f\"Number of {word}s on {website1_name} front page: {len(prices_website1)}\")\n",
    "    print(f\"Average price on {website1_name}: ${average_website1:.2f}\")\n",
    "    min_price, max_price = get_min_max(prices_website1)\n",
    "    if min_price and max_price:\n",
    "        print(f\"Lowest price on {website1_name}: {min_price['name']} at ${min_price['price']:.2f}\")\n",
    "        print(f\"Highest price on {website1_name}: {max_price['name']} at ${max_price['price']:.2f}\")\n",
    "    print(f\"Average rating on {website1_name}: {average_rating_website1:.2f}\")\n",
    "\n",
    "    print(f\"\\nNumber of {word}s on {website2_name} front page: {len(prices_website2)}\")\n",
    "    print(f\"Average price on {website2_name}: ${average_website2:.2f}\")\n",
    "    min_price, max_price = get_min_max(prices_website2)\n",
    "    if min_price and max_price:\n",
    "        print(f\"Lowest price on {website2_name}: {min_price['name']} at ${min_price['price']:.2f}\")\n",
    "        print(f\"Highest price on {website2_name}: {max_price['name']} at ${max_price['price']:.2f}\")\n",
    "    print(f\"Average rating on {website2_name}: {average_rating_website2:.2f}\")\n",
    "\n",
    "    price_diff = abs(average_website1 - average_website2)\n",
    "    print(f\"\\nPrice Comparison: {website1_name} is {'cheaper' if average_website1 < average_website2 else 'more expensive' if average_website1 > average_website2 else 'equally priced'} than {website2_name} by ${price_diff:.2f}\")\n",
    "\n",
    "    # Process website 1 prices\n",
    "    min_price_website1, max_price_website1 = get_min_max(prices_website1)\n",
    "    # Process website 2 prices\n",
    "    min_price_website2, max_price_website2 = get_min_max(prices_website2)\n",
    "\n",
    "    current_date = date.today().strftime('%m/%d/%Y')\n",
    "\n",
    "\n",
    "    # After authorizing gspread\n",
    "    sheet = client.open_by_url('URL_FOR_GOOGLE_SHEET')\n",
    "\n",
    "    # Check if worksheet exists, create if not\n",
    "    try:\n",
    "        item_prices_ws = sheet.worksheet('item_prices') \n",
    "    except WorksheetNotFound:\n",
    "        item_prices_ws = sheet.add_worksheet(title=\"item_prices\", rows=\"1\", cols=\"8\")\n",
    "            \n",
    "    try:\n",
    "        compared_prices_ws = sheet.worksheet('compared_prices') \n",
    "    except WorksheetNotFound:\n",
    "        compared_prices_ws = sheet.add_worksheet(title=\"compared_prices\", rows=\"1\", cols=\"10\")\n",
    "\n",
    "    # Check if the sheets are empty before writing headers\n",
    "    if len(item_prices_ws.get_all_values()) == 0:\n",
    "        item_prices_ws.append_row(['ID', 'SearchID', 'Website', 'Item', 'Name', 'Price', 'Rating', 'Date Added'])\n",
    "    if len(compared_prices_ws.get_all_values()) == 0:\n",
    "        compared_prices_ws.append_row(['ID', 'SearchID', 'Website', 'Item', 'Lowest Price', 'Highest Price', 'Avg Price', 'Price Difference', 'Avg Rating', 'Date Added'])\n",
    "\n",
    "    # First Google Sheet\n",
    "    item_prices = [['SearchID', 'Website', 'Item', 'Name', 'Price', 'Rating', 'Date Added']]\n",
    "    # Generate unique IDs for each item\n",
    "    for i, item in enumerate(prices_website1, start=item_prices_max_id+1):\n",
    "        item_prices.append([i, search_id, website1_name, word, item['name'], item['price'], item['rating'] if item['rating'] is not None else 0, current_date])\n",
    "    for i, item in enumerate(prices_website2, start=len(prices_website1)+item_prices_max_id+1):\n",
    "        item_prices.append([i, search_id, website2_name, word, item['name'], item['price'], item['rating'] if item['rating'] is not None else 0, current_date])\n",
    "\n",
    "\n",
    "    # Second Google Sheet\n",
    "    compared_prices = [['SearchID', 'Website', 'Item', 'Lowest Price', 'Highest Price', 'Avg Price', 'Price Difference', 'Avg Rating', 'Date Added']]\n",
    "    if prices_website1:\n",
    "        compared_prices.append([compared_prices_max_id+1, search_id, website1_name, word, min_price_website1['price'], max_price_website1['price'], average_website1, price_diff, average_rating_website1 if average_rating_website1 is not None else 0, current_date])\n",
    "    if prices_website2:\n",
    "        compared_prices.append([compared_prices_max_id+2, search_id, website2_name, word, min_price_website2['price'], max_price_website2['price'], average_website2, price_diff, average_rating_website2 if average_rating_website2 is not None else 0, current_date])\n",
    "\n",
    "\n",
    "    # Append rows to item_prices\n",
    "    for row in item_prices[1:]:  # Skip the header\n",
    "        item_prices_ws.append_row(row)\n",
    "\n",
    "    # Append rows to compared_prices\n",
    "    for row in compared_prices[1:]:  # Skip the header\n",
    "        compared_prices_ws.append_row(row)\n",
    "\n",
    "    print(f\"\\nGoogle Sheet Updated\")\n",
    "\n",
    "search_id = get_search_id()\n",
    "item_prices_max_id = get_max_id('item_prices')\n",
    "compared_prices_max_id = get_max_id('compared_prices')\n",
    "search_word = get_search_word_from_sheet()\n",
    "\n",
    "if search_word is not None:\n",
    "    compare_prices(search_word, item_prices_max_id, compared_prices_max_id)\n",
    "else:\n",
    "    print(\"No search word for today was found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for setting up local mysql server\n",
    "\n",
    "from datetime import datetime\n",
    "import gspread\n",
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "\n",
    "# Define the scope\n",
    "scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']\n",
    "\n",
    "# Add your Service Account File\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_name(r\"PATH_TO_CREDENTIALS_FILE\", scope)\n",
    "\n",
    "# authorize your client\n",
    "client = gspread.authorize(creds)\n",
    "\n",
    "def create_database():\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = mysql.connector.connect(\n",
    "            host=\"localhost\",\n",
    "            user=\"root\",\n",
    "            passwd=\"admin\"\n",
    "        )\n",
    "        if conn.is_connected():\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(\"CREATE DATABASE PricesDB\")\n",
    "            print('Database created successfully....')\n",
    "    except Error as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "create_database()\n",
    "\n",
    "\n",
    "def create_connection():\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = mysql.connector.connect(\n",
    "            host=\"localhost\",\n",
    "            user=\"root\",\n",
    "            passwd=\"admin\",\n",
    "            database=\"PricesDB\"\n",
    "        )\n",
    "        if conn.is_connected():\n",
    "            print('Connected to MySQL database')\n",
    "    except Error as e:\n",
    "        print(e)\n",
    "    return conn\n",
    "\n",
    "\n",
    "def create_table(conn):\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        item_prices_table_query = \"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS item_prices (\n",
    "                SEARCHID NUMBER(38,0),\n",
    "                WEBSITE VARCHAR(255),\n",
    "                ITEM VARCHAR(255),\n",
    "                NAME VARCHAR(255),\n",
    "                PRICE FLOAT,\n",
    "                RATING FLOAT,\n",
    "                DATEADDED DATE\n",
    "            );\n",
    "        \"\"\"\n",
    "        cursor.execute(item_prices_table_query)\n",
    "\n",
    "        compared_prices_table_query = \"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS compared_prices (\n",
    "                SEARCHID NUMBER(38,0),\n",
    "                WEBSITE VARCHAR(255),\n",
    "                ITEM VARCHAR(255),\n",
    "                LOWESTPRICE FLOAT,\n",
    "                HIGHESTPRICE FLOAT,\n",
    "                AVGPRICE FLOAT,\n",
    "                PRICEDIFFERENCE FLOAT,\n",
    "                AVGRATING FLOAT,\n",
    "                DATEADDED DATE\n",
    "            );\n",
    "        \"\"\"\n",
    "        cursor.execute(compared_prices_table_query)\n",
    "\n",
    "        print('Tables Updated...')\n",
    "\n",
    "    except Error as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "def search_id_exists(conn, search_id, table_name):\n",
    "    query = f\"SELECT 1 FROM {table_name} WHERE SearchID = {float(search_id)} LIMIT 1\"\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(query)\n",
    "        return cursor.fetchone() is not None\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return False\n",
    "    \n",
    "def get_max_search_id(conn, table_name):\n",
    "    query = f\"SELECT MAX(SearchID) FROM {table_name}\"\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(query)\n",
    "        max_search_id = cursor.fetchone()[0]\n",
    "        return max_search_id if max_search_id else 0\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return 0\n",
    "\n",
    "\n",
    "def insert_into_item_prices(conn, data):\n",
    "    query = \"\"\"\n",
    "        INSERT INTO item_prices(SearchID, Website, Item, Name, Price, Rating, DateAdded)\n",
    "        VALUES (%s, %s, %s, %s, %s, %s, %s)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Get the maximum existing SearchID\n",
    "        max_search_id = get_max_search_id(conn, \"item_prices\")\n",
    "        \n",
    "        # Filter out data with a SearchID higher than the maximum value\n",
    "        filtered_data = [(row[0], row[1], row[2], row[3], row[4], row[5], row[6]) for row in data if int(row[0]) > max_search_id]\n",
    "        \n",
    "        if filtered_data:\n",
    "            cursor.executemany(query, filtered_data)\n",
    "            conn.commit()\n",
    "            print(f\"Inserted {len(filtered_data)} new rows into item_prices.\")\n",
    "        else:\n",
    "            print(\"No new data to insert into item_prices.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "def insert_into_compared_prices(conn, data):\n",
    "    query = \"\"\"\n",
    "        INSERT INTO compared_prices(SearchID, Website, Item, LowestPrice, HighestPrice, AvgPrice, PriceDifference, AvgRating, DateAdded)\n",
    "        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Get the maximum existing SearchID\n",
    "        max_search_id = get_max_search_id(conn, \"compared_prices\")\n",
    "        \n",
    "        # Filter out data with a SearchID higher than the maximum value\n",
    "        filtered_data = [(row[0], row[1], row[2], row[3], row[4], row[5], row[6], row[7], row[8]) for row in data if int(row[0]) > max_search_id]\n",
    "        \n",
    "        if filtered_data:\n",
    "            cursor.executemany(query, filtered_data)\n",
    "            conn.commit()\n",
    "            print(f\"Inserted {len(filtered_data)} new rows into compared_prices.\")\n",
    "        else:\n",
    "            print(\"No new data to insert into compared_prices.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "\n",
    "def close_connection(conn):\n",
    "    if conn:\n",
    "        conn.close()\n",
    "        print('Database connection closed.')\n",
    "\n",
    "\n",
    "# Establish the connection\n",
    "conn = create_connection()\n",
    "\n",
    "# Create tables if they don't exist\n",
    "create_table(conn)\n",
    "\n",
    "\n",
    "def read_data_from_google_sheet(sheet_url):\n",
    "    \n",
    "    # Open the Google spreadsheet using its url\n",
    "    sheet = client.open_by_url(sheet_url)\n",
    "    \n",
    "    # Select the first sheet in the spreadsheet\n",
    "    worksheet = sheet.get_worksheet(0)\n",
    "    \n",
    "    # Get all data from the worksheet\n",
    "    data = worksheet.get_all_values()\n",
    "    return data\n",
    "\n",
    "def read_gsheet_and_insert_item_prices(sheet_url, insert_function):\n",
    "    data = read_data_from_google_sheet(sheet_url)\n",
    "    items = []\n",
    "    unique_search_ids = set()\n",
    "    for row in data[1:]:  # Skip header row\n",
    "        # Convert price related data to float and date to proper date format\n",
    "        for i in [5]:  # Convert 'Price' to float\n",
    "            row[i] = float(row[i])\n",
    "        if '/' in row[7]:  # Convert 'Date Added' to date format\n",
    "            row[7] = datetime.strptime(row[7], '%m/%d/%Y').date()\n",
    "        items.append(row)\n",
    "        unique_search_ids.add(row[1])\n",
    "\n",
    "    items_to_insert = []  # Initialize the list here\n",
    "\n",
    "    for search_id in unique_search_ids:\n",
    "        if search_id_exists(conn, search_id, \"item_prices\"):\n",
    "            print(f\"SearchID {search_id} already exists. Skipping...\")\n",
    "            continue\n",
    "        for item in items: # replace 'sorted_items' with 'items'\n",
    "            if item[1] == search_id:\n",
    "                items_to_insert.append(tuple(item[1:]))\n",
    "\n",
    "    insert_function(conn, items_to_insert)\n",
    "\n",
    "def read_gsheet_and_insert_compared_prices(sheet_url, insert_function):\n",
    "    # Open the Google spreadsheet using its URL\n",
    "    sheet = client.open_by_url(sheet_url)\n",
    "\n",
    "    # Select the second sheet in the spreadsheet (index 1)\n",
    "    worksheet = sheet.get_worksheet(1)\n",
    "\n",
    "    # Get all data from the worksheet\n",
    "    data = worksheet.get_all_values()\n",
    "\n",
    "    # Find the index of the ID column in the header row\n",
    "    header_row = data[0]\n",
    "    id_column_index = header_row.index('ID')\n",
    "\n",
    "    items = []\n",
    "    unique_search_ids = set()\n",
    "    for row in data[1:]:  # Skip header row\n",
    "        # Convert price related data to float and date to proper date format\n",
    "        if len(row) >= 8:  # Check if row has at least 8 columns\n",
    "            for i in range(4, 8):  # Convert columns 4 to 7 to float\n",
    "                row[i] = float(row[i])\n",
    "            if '/' in row[9]:  # Convert 'Date Added' to date format\n",
    "                row[9] = datetime.strptime(row[9], '%m/%d/%Y').date()\n",
    "            items.append(row)\n",
    "            unique_search_ids.add(row[1])\n",
    "\n",
    "    items_to_insert = []\n",
    "\n",
    "    for search_id in unique_search_ids:\n",
    "        if search_id_exists(conn, search_id, \"compared_prices\"):\n",
    "            print(f\"SearchID {search_id} already exists. Skipping...\")\n",
    "            continue\n",
    "        for item in items: # replace 'sorted_items' with 'items'\n",
    "            if item[1] == search_id:\n",
    "                items_to_insert.append(tuple(item[1:]))\n",
    "                \n",
    "    insert_function(conn, items_to_insert)\n",
    "\n",
    "\n",
    "# Establish the Snowflake connection\n",
    "conn = create_connection()\n",
    "\n",
    "# Create tables if they don't exist\n",
    "create_table(conn)\n",
    "\n",
    "# Read and insert data for each sheet\n",
    "read_gsheet_and_insert_item_prices('URL_FOR_GOOGLE_SHEET', insert_into_item_prices)\n",
    "read_gsheet_and_insert_compared_prices('URL_FOR_GOOGLE_SHEET', insert_into_compared_prices)\n",
    "\n",
    "print(f\"Data Upload Completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for setting up and sending data to Snowflake DB\n",
    "\n",
    "# Adding data to our Snowflake Database\n",
    "def create_connection():\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = snowflake.connector.connect(\n",
    "            user=\"USERNAME\",\n",
    "            password=\"PASSWORD\",\n",
    "            account=\"SNOWFLAKE IDENTIFIER\"\n",
    "        )\n",
    "        print('Connected to Snowflake database')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    return conn\n",
    "\n",
    "\n",
    "def create_table(conn):\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"USE DATABASE PRICE_DATABASE\")\n",
    "        item_prices_table_query = \"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS PRICE_DATABASE.PUBLIC.ITEM_PRICES (\n",
    "                SEARCHID NUMBER(38,0),\n",
    "                WEBSITE VARCHAR(16777216),\n",
    "                ITEM VARCHAR(16777216),\n",
    "                NAME VARCHAR(16777216),\n",
    "                PRICE FLOAT,\n",
    "                RATING FLOAT,\n",
    "                DATEADDED DATE\n",
    "            );\n",
    "        \"\"\"\n",
    "        cursor.execute(item_prices_table_query)\n",
    "\n",
    "        compared_prices_table_query = \"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS PRICE_DATABASE.PUBLIC.COMPARED_PRICES (\n",
    "                SEARCHID NUMBER(38,0),\n",
    "                WEBSITE VARCHAR(16777216),\n",
    "                ITEM VARCHAR(16777216),\n",
    "                LOWESTPRICE FLOAT,\n",
    "                HIGHESTPRICE FLOAT,\n",
    "                AVGPRICE FLOAT,\n",
    "                PRICEDIFFERENCE FLOAT,\n",
    "                AVGRATING FLOAT,\n",
    "                DATEADDED DATE\n",
    "            );\n",
    "        \"\"\"\n",
    "        cursor.execute(compared_prices_table_query)\n",
    "\n",
    "        print('Tables Updated...')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "def search_id_exists(conn, search_id, table_name):\n",
    "    query = f\"SELECT 1 FROM {table_name} WHERE SearchID = {float(search_id)} LIMIT 1\"\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(query)\n",
    "        return cursor.fetchone() is not None\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return False\n",
    "    \n",
    "def get_max_search_id(conn, table_name):\n",
    "    query = f\"SELECT MAX(SearchID) FROM {table_name}\"\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(query)\n",
    "        max_search_id = cursor.fetchone()[0]\n",
    "        return max_search_id if max_search_id else 0\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return 0\n",
    "\n",
    "\n",
    "def insert_into_item_prices(conn, data):\n",
    "    query = \"\"\"\n",
    "        INSERT INTO item_prices(SearchID, Website, Item, Name, Price, Rating, DateAdded)\n",
    "        VALUES (%s, %s, %s, %s, %s, %s, %s)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Get the maximum existing SearchID\n",
    "        max_search_id = get_max_search_id(conn, \"item_prices\")\n",
    "        \n",
    "        # Filter out data with a SearchID higher than the maximum value\n",
    "        filtered_data = [(row[0], row[1], row[2], row[3], row[4], row[5], row[6]) for row in data if int(row[0]) > max_search_id]\n",
    "        \n",
    "        if filtered_data:\n",
    "            cursor.executemany(query, filtered_data)\n",
    "            conn.commit()\n",
    "            print(f\"Inserted {len(filtered_data)} new rows into item_prices.\")\n",
    "        else:\n",
    "            print(\"No new data to insert into item_prices.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "def insert_into_compared_prices(conn, data):\n",
    "    query = \"\"\"\n",
    "        INSERT INTO compared_prices(SearchID, Website, Item, LowestPrice, HighestPrice, AvgPrice, PriceDifference, AvgRating, DateAdded)\n",
    "        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Get the maximum existing SearchID\n",
    "        max_search_id = get_max_search_id(conn, \"compared_prices\")\n",
    "        \n",
    "        # Filter out data with a SearchID higher than the maximum value\n",
    "        filtered_data = [(row[0], row[1], row[2], row[3], row[4], row[5], row[6], row[7], row[8]) for row in data if int(row[0]) > max_search_id]\n",
    "        \n",
    "        if filtered_data:\n",
    "            cursor.executemany(query, filtered_data)\n",
    "            conn.commit()\n",
    "            print(f\"Inserted {len(filtered_data)} new rows into compared_prices.\")\n",
    "        else:\n",
    "            print(\"No new data to insert into compared_prices.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "def close_connection(conn):\n",
    "    if conn:\n",
    "        conn.close()\n",
    "        print('Database connection closed.')\n",
    "\n",
    "\n",
    "# Establish the Snowflake connection\n",
    "conn = create_connection()\n",
    "\n",
    "# Create tables if they don't exist\n",
    "create_table(conn)\n",
    "\n",
    "\n",
    "def read_data_from_google_sheet(sheet_url):\n",
    "    \n",
    "    # Open the Google spreadsheet using its url\n",
    "    sheet = client.open_by_url(sheet_url)\n",
    "    \n",
    "    # Select the first sheet in the spreadsheet\n",
    "    worksheet = sheet.get_worksheet(0)\n",
    "    \n",
    "    # Get all data from the worksheet\n",
    "    data = worksheet.get_all_values()\n",
    "    return data\n",
    "\n",
    "def read_gsheet_and_insert_item_prices(sheet_url, insert_function):\n",
    "    data = read_data_from_google_sheet(sheet_url)\n",
    "    items = []\n",
    "    unique_search_ids = set()\n",
    "    for row in data[1:]:  # Skip header row\n",
    "        # Convert price related data to float and date to proper date format\n",
    "        for i in [5]:  # Convert 'Price' to float\n",
    "            row[i] = float(row[i])\n",
    "        if '/' in row[7]:  # Convert 'Date Added' to date format\n",
    "            row[7] = datetime.strptime(row[7], '%m/%d/%Y').date()\n",
    "        items.append(row)\n",
    "        unique_search_ids.add(row[1])\n",
    "\n",
    "    items_to_insert = []  # Initialize the list here\n",
    "\n",
    "    for search_id in unique_search_ids:\n",
    "        if search_id_exists(conn, search_id, \"item_prices\"):\n",
    "            print(f\"SearchID {search_id} already exists. Skipping...\")\n",
    "            continue\n",
    "        for item in items: # replace 'sorted_items' with 'items'\n",
    "            if item[1] == search_id:\n",
    "                items_to_insert.append(tuple(item[1:]))\n",
    "\n",
    "    insert_function(conn, items_to_insert)\n",
    "\n",
    "def read_gsheet_and_insert_compared_prices(sheet_url, insert_function):\n",
    "    # Open the Google spreadsheet using its URL\n",
    "    sheet = client.open_by_url(sheet_url)\n",
    "\n",
    "    # Select the second sheet in the spreadsheet (index 1)\n",
    "    worksheet = sheet.get_worksheet(1)\n",
    "\n",
    "    # Get all data from the worksheet\n",
    "    data = worksheet.get_all_values()\n",
    "\n",
    "    # Find the index of the ID column in the header row\n",
    "    header_row = data[0]\n",
    "    id_column_index = header_row.index('ID')\n",
    "\n",
    "    items = []\n",
    "    unique_search_ids = set()\n",
    "    for row in data[1:]:  # Skip header row\n",
    "        # Convert price related data to float and date to proper date format\n",
    "        if len(row) >= 8:  # Check if row has at least 8 columns\n",
    "            for i in range(4, 8):  # Convert columns 4 to 7 to float\n",
    "                row[i] = float(row[i])\n",
    "            if '/' in row[9]:  # Convert 'Date Added' to date format\n",
    "                row[9] = datetime.strptime(row[9], '%m/%d/%Y').date()\n",
    "            items.append(row)\n",
    "            unique_search_ids.add(row[1])\n",
    "\n",
    "    items_to_insert = []\n",
    "\n",
    "    for search_id in unique_search_ids:\n",
    "        if search_id_exists(conn, search_id, \"compared_prices\"):\n",
    "            print(f\"SearchID {search_id} already exists. Skipping...\")\n",
    "            continue\n",
    "        for item in items: # replace 'sorted_items' with 'items'\n",
    "            if item[1] == search_id:\n",
    "                items_to_insert.append(tuple(item[1:]))\n",
    "                \n",
    "    insert_function(conn, items_to_insert)\n",
    "\n",
    "\n",
    "# Establish the Snowflake connection\n",
    "conn = create_connection()\n",
    "\n",
    "# Create tables if they don't exist\n",
    "create_table(conn)\n",
    "\n",
    "# Read and insert data for each sheet\n",
    "read_gsheet_and_insert_item_prices('URL_FOR_GOOGLE_SHEET', insert_into_item_prices)\n",
    "read_gsheet_and_insert_compared_prices('URL_FOR_GOOGLE_SHEET', insert_into_compared_prices)\n",
    "\n",
    "print(f\"Data Upload Completed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
