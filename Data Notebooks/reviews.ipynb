{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for pulling the Reveiws from the two websites we are comparing prices for\n",
    "\n",
    "## Code for the price comparison is [HERE](https://github.com/CameronCSS/Programming-Languages/blob/main/Data%20Notebooks/Price%20Comparison.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from gspread.exceptions import WorksheetNotFound\n",
    "\n",
    "current_date = date.today().strftime('%m/%d/%Y')\n",
    "\n",
    "# Define the scope\n",
    "scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']\n",
    "\n",
    "# Add your Service Account File\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_name(r\"PATH_TO_CREDENTIALS\", scope)\n",
    "\n",
    "# Authorize your client\n",
    "client = gspread.authorize(creds)\n",
    "\n",
    "def get_search_word_from_sheet():\n",
    "    # Open the Google Spreadsheet by its URL (make sure you have access to it)\n",
    "    sheet = client.open_by_url('GOOGLE_SHEETS_URL').sheet1\n",
    "\n",
    "    # Get all the records of the data\n",
    "    data = sheet.get_all_records()\n",
    "\n",
    "    # Get today's date if searching by Date instead of the day of the week\n",
    "    today = date.today()\n",
    "\n",
    "    # Checks for date instead of the day of the week\n",
    "    for row in data:\n",
    "        # If the date in the 'date' column matches today's date, return the corresponding search word\n",
    "        date_from_sheet = datetime.strptime(row['date'], '%m/%d/%Y').date()\n",
    "        if date_from_sheet == today:\n",
    "            return row['search_word']\n",
    "\n",
    "    return None\n",
    "\n",
    "def scrape_website(url, word):\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "\n",
    "    if \"livingspaces.com\" in url:\n",
    "        input_field = driver.find_element(By.ID, 'search')\n",
    "        input_field.send_keys(word)\n",
    "        form = input_field.find_element(By.XPATH, './ancestor::form')\n",
    "        form.submit()\n",
    "\n",
    "    elif \"rcwilley.com\" in url:\n",
    "        input_field = driver.find_element(By.ID, 'searchBox')\n",
    "        input_field.send_keys(word)\n",
    "        submit_button = driver.find_element(By.ID, 'searchSubmit')\n",
    "        submit_button.click()\n",
    "\n",
    "    time.sleep(5)  \n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    reviews = []\n",
    "\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "\n",
    "    if \"livingspaces.com\" in url:\n",
    "        product_items = driver.find_elements(By.CLASS_NAME, 'product-item-container')\n",
    "        product_urls = [] # We store the URLs here\n",
    "\n",
    "        # Collect all the product URLs that have a rating\n",
    "        for item in product_items:\n",
    "            try:\n",
    "                name_element = item.find_element(By.CLASS_NAME, 'name')\n",
    "                rating_element = item.find_element(By.CLASS_NAME, 'ratings')\n",
    "                product_link_element = item.find_element(By.TAG_NAME, 'a')\n",
    "                \n",
    "                if name_element and rating_element and product_link_element:\n",
    "                    name = name_element.text.strip()\n",
    "                    product_link = product_link_element.get_attribute('href')\n",
    "                    sku = product_link[-6:] # Get SKU from the product link\n",
    "                    \n",
    "                    # Keep only digits in SKU\n",
    "                    sku = ''.join(filter(str.isdigit, sku))\n",
    "\n",
    "                    rating = 0\n",
    "                    rating_text = rating_element.get_attribute('aria-label')\n",
    "                    if rating_text:\n",
    "                        rating = float(rating_text.split(' out of ')[0])\n",
    "                    rating = int(rating) if isinstance(rating, float) and rating.is_integer() else rating \n",
    "\n",
    "                    # Append product link and other details to the list\n",
    "                    product_urls.append((product_link, name, rating, sku)) # Added SKU in the tuple\n",
    "                \n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        # Now we iterate over all URLs and repeat the process for each one\n",
    "        for product_url, name, rating, sku in product_urls: # Updated unpacking with SKU\n",
    "            driver.get(product_url)\n",
    "            time.sleep(3)\n",
    "\n",
    "            wait = WebDriverWait(driver, 10)\n",
    "\n",
    "            # Use the wait object to wait until the button is clickable, then click it.\n",
    "            try:\n",
    "                button = wait.until(EC.element_to_be_clickable((By.XPATH, \"//div[@role='button' and contains(@class, 'ratings')]\")))\n",
    "                button.click()\n",
    "                time.sleep(2)\n",
    "            except:\n",
    "                continue\n",
    "                    \n",
    "            # Get the page source and pass it into BeautifulSoup\n",
    "            # This is required. I could not get Selenium to understand the HTML in the way bs4 can.\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "            # Find individual reviews and ratings using BeautifulSoup\n",
    "            review_items = soup.select('div.bvseo-review')\n",
    "\n",
    "            for review in review_items:\n",
    "                try:\n",
    "                    # Extract the review text and rating score\n",
    "                    review_text = review.find('span', itemprop='description').text.strip()\n",
    "                    rating_score = int(review.find('span', itemprop='ratingValue').text.strip())\n",
    "                    reviews.append([sku, name, rating, review_text, current_date])\n",
    "                except:\n",
    "                    continue\n",
    "        print(f\"Livingspaces Reviews Added\")\n",
    "\n",
    "    elif \"rcwilley.com\" in url:\n",
    "        product_items = soup.find_all('div', class_='productContent')\n",
    "        for item in product_items:\n",
    "            rating_element = item.find('div', class_='rating')\n",
    "            name_element = item.find('div', class_='productName')\n",
    "            if name_element:\n",
    "                name = name_element.text.strip()\n",
    "                rating_element = item.find('div', class_=lambda value: value and value.startswith('rating'))\n",
    "                rating = 0\n",
    "                if rating_element:\n",
    "                    rating_span = rating_element.find('span', class_='sr-only')\n",
    "                    if rating_span:\n",
    "                        rating_text = rating_span.text.strip()\n",
    "                        rating = float(''.join([i for i in rating_text if i.isdigit() or i == '.']))\n",
    "                product_link_element = item.find_parent('a')  # Find the parent `a` tag of the current item.\n",
    "                rating = int(rating) if isinstance(rating, float) and rating.is_integer() else rating\n",
    "                if product_link_element:\n",
    "                    product_link = \"https://www.rcwilley.com\" + product_link_element.get('href')\n",
    "\n",
    "                    # Here we extract SKU from the product_link_element\n",
    "                    sku = product_link_element.get('id')\n",
    "                    if sku.startswith('sku-'):\n",
    "                        sku = sku[4:]  # If the sku starts with 'sku-', strip 'sku-' prefix\n",
    "\n",
    "                    # Check if the product has a rating\n",
    "                    if rating > 0:\n",
    "                        # Navigate to the product link without the reviews tab\n",
    "                        driver.get(product_link)\n",
    "                        time.sleep(3)\n",
    "\n",
    "                        # Update the product link to include the reviews tab\n",
    "                        product_link += \"#reviews-tab\"\n",
    "\n",
    "                        # Navigate to the updated product link with the reviews tab\n",
    "                        driver.get(product_link)\n",
    "                        time.sleep(2)\n",
    "\n",
    "                        # Scroll to the reviews tab\n",
    "                        driver.execute_script(\"arguments[0].scrollIntoView(true);\", driver.find_element(By.XPATH, \"//a[@href='#reviews-tab']\"))\n",
    "\n",
    "                        # Click on the reviews tab using JavaScript\n",
    "                        driver.execute_script(\"arguments[0].click();\", driver.find_element(By.XPATH, \"//a[@href='#reviews-tab']\"))\n",
    "\n",
    "                        # Wait for the reviews to load\n",
    "                        time.sleep(3)\n",
    "\n",
    "                        # Get the page source and pass it into BeautifulSoup\n",
    "                        # This is required. I could not get Selenium to understand the HTML in the way bs4 can. Especially with it being mixed HTML and JSON\n",
    "                        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "                        try:\n",
    "                            jsonld_script = soup.find('script', {'id': 'bv-jsonld-reviews-data'}).string\n",
    "                            jsonld_data = json.loads(jsonld_script)\n",
    "                            # Extract the review body text and rating score for each review\n",
    "                            # Process the reviews as needed\n",
    "                        except AttributeError:\n",
    "                            continue\n",
    "\n",
    "                        # Extract the review body text and rating score for each review\n",
    "                        for review_data in jsonld_data['review']:\n",
    "                            review_body = review_data['reviewBody']\n",
    "                            rating_score = review_data['reviewRating']['ratingValue']\n",
    "\n",
    "                            # Print the review body and rating score\n",
    "                            try:\n",
    "                                reviews.append([sku, name, rating_score, review_body, current_date])  # updated review list to include SKU\n",
    "                            except:\n",
    "                                continue\n",
    "        print(f\"RC Willey Reviews Added\")\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    return reviews\n",
    "\n",
    "# Double check the IDs and continue counting\n",
    "def get_max_id(sheet_name):\n",
    "    # Authorize your client\n",
    "    client = gspread.authorize(creds)\n",
    "\n",
    "    # Open the Google Spreadsheet by its URL\n",
    "    sheet = client.open_by_url('GOOGLE_SHEETS_URL')\n",
    "\n",
    "    try:\n",
    "        ws = sheet.worksheet(sheet_name)\n",
    "        data = ws.get_all_values()  # Get all values inside the specified worksheet\n",
    "        if data:\n",
    "            # The first column has ID, get the max value\n",
    "            ids = [int(row[0]) for row in data[1:] if row[0].isdigit()]  # Ignore the first row (header)\n",
    "            if ids:\n",
    "                return max(ids)\n",
    "    except WorksheetNotFound:\n",
    "        pass\n",
    "    return 0\n",
    "\n",
    "def compare_reviews(word):\n",
    "    website1_url = 'https://www.livingspaces.com/'\n",
    "    website2_url = 'https://www.rcwilley.com/'\n",
    "\n",
    "    reviews_website1 = scrape_website(website1_url, word)\n",
    "    reviews_website2 = scrape_website(website2_url, word)\n",
    "\n",
    "    website1_name = website1_url.replace('https://www.', '').replace('.com/', '').capitalize()\n",
    "    website2_name = website2_url.replace('https://www.', '').replace('.com/', '').capitalize()\n",
    "\n",
    "    sheet = client.open_by_url('GOOGLE_SHEETS_URL')\n",
    "\n",
    "\n",
    "    # Check if reviews sheet exists\n",
    "    try:\n",
    "        reviews_ws = sheet.worksheet('reviews')\n",
    "    except WorksheetNotFound:\n",
    "        reviews_ws = sheet.add_worksheet(title=\"reviews\", rows=\"1\", cols=\"5\")\n",
    "\n",
    "    # Check if the reviews sheet is empty before writing headers\n",
    "    if len(reviews_ws.get_all_values()) == 0:\n",
    "        reviews_ws.append_row(['Website', 'Sku', 'Rating', 'Review', 'Date Added'])\n",
    "\n",
    "\n",
    "    # Generate unique IDs for each item\n",
    "    reviews_max_id = get_max_id('reviews')\n",
    "\n",
    "\n",
    "    # Reviews for livingspaces.com\n",
    "    reviews_rows = []\n",
    "    for i, (sku, name, rating, review_text, _) in enumerate(reviews_website1, start=reviews_max_id+1):\n",
    "        reviews_rows.append([website1_name, sku, rating, review_text, current_date])\n",
    "\n",
    "    reviews_ws.append_rows(reviews_rows)\n",
    "\n",
    "\n",
    "    # Reviews for rcwilley.com\n",
    "    reviews_rows = []\n",
    "    for i, (sku, name, rating, review_text, _) in enumerate(reviews_website2, start=reviews_max_id+1):\n",
    "        reviews_rows.append([website2_name, sku, rating, review_text, current_date])\n",
    "\n",
    "    reviews_ws.append_rows(reviews_rows)\n",
    "\n",
    "    print(\"Google Sheets Updated\")\n",
    "\n",
    "\n",
    "# To get search word from google sheet\n",
    "search_word = get_search_word_from_sheet()\n",
    "\n",
    "if search_word is not None:\n",
    "    compare_reviews(search_word)\n",
    "else:\n",
    "    print(\"No search word for today was found.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
