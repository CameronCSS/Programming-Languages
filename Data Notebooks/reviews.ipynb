{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for pulling the Reveiws from the two websites we are comparing prices for
    \n Code for the price comparison is [HERE](https://github.com/CameronCSS/Programming-Languages/blob/main/Data%20Notebooks/Price%20Comparison.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "def scrape_website(url, word):\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "\n",
    "    if \"livingspaces.com\" in url:\n",
    "        input_field = driver.find_element(By.ID, 'search')\n",
    "        input_field.send_keys(word)\n",
    "        form = input_field.find_element(By.XPATH, './ancestor::form')\n",
    "        form.submit()\n",
    "\n",
    "    elif \"rcwilley.com\" in url:\n",
    "        input_field = driver.find_element(By.ID, 'searchBox')\n",
    "        input_field.send_keys(word)\n",
    "        submit_button = driver.find_element(By.ID, 'searchSubmit')\n",
    "        submit_button.click()\n",
    "\n",
    "    time.sleep(5)  \n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    prices = []\n",
    "\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "\n",
    "    if \"livingspaces.com\" in url:\n",
    "        product_items = driver.find_elements(By.CLASS_NAME, 'product-item-container')\n",
    "        for i, item in enumerate(product_items[:1]):\n",
    "            rating_element = item.find_element(By.CLASS_NAME, 'ratings')\n",
    "            if rating_element:\n",
    "                product_link_element = item.find_element(By.TAG_NAME, 'a')\n",
    "                if product_link_element:\n",
    "                    product_link = product_link_element.get_attribute('href')\n",
    "                    driver.get(product_link)\n",
    "                    time.sleep(3)\n",
    "                    \n",
    "                    wait = WebDriverWait(driver, 10)\n",
    "\n",
    "                    # Use the wait object to wait until the button is clickable, then click it.\n",
    "                    button = wait.until(EC.element_to_be_clickable((By.XPATH, \"//div[@role='button' and contains(@class, 'ratings')]\")))\n",
    "                    button.click()\n",
    "                    time.sleep(2)\n",
    "                    \n",
    "                    # Get the page source and pass it into BeautifulSoup\n",
    "                    # This is requred. I could not get Selenium to understand the HTML in the way bs4 can.\n",
    "                    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "                    # Find individual reviews using BeautifulSoup\n",
    "                    individual_reviews = soup.select('span[itemprop=\"description\"]')\n",
    "\n",
    "                    for review in individual_reviews:\n",
    "                        try:\n",
    "                            # Print the review text\n",
    "                            review_text = review.text.strip()\n",
    "                            print(f\"Review: {review_text}\")\n",
    "                        except:\n",
    "                            print(f\"Couldn't Get Reviews\")\n",
    "\n",
    "    elif \"rcwilley.com\" in url:\n",
    "        product_items = soup.find_all('div', class_='productContent')\n",
    "        for i, item in enumerate(product_items[:2]):\n",
    "            rating_element = item.find('div', class_='rating')\n",
    "            if rating_element:\n",
    "                if rating_element and rating_element.text.strip():  # Check if the rating element contains text\n",
    "                    product_link_element = item.find_parent('a')  # Find the parent `a` tag of the current item.\n",
    "                    if product_link_element:\n",
    "                        product_link = \"https://www.rcwilley.com\" + product_link_element.get('href')\n",
    "\n",
    "                        # Navigate to the product link without the reviews tab\n",
    "                        driver.get(product_link)\n",
    "                        time.sleep(3)\n",
    "\n",
    "                        # Update the product link to include the reviews tab\n",
    "                        product_link += \"#reviews-tab\"\n",
    "\n",
    "                        # Navigate to the updated product link with the reviews tab\n",
    "                        driver.get(product_link)\n",
    "                        time.sleep(2)\n",
    "\n",
    "                        # Scroll to the reviews tab\n",
    "                        driver.execute_script(\"arguments[0].scrollIntoView(true);\", driver.find_element(By.XPATH, \"//a[@href='#reviews-tab']\"))\n",
    "\n",
    "                        # Click on the reviews tab using JavaScript\n",
    "                        driver.execute_script(\"arguments[0].click();\", driver.find_element(By.XPATH, \"//a[@href='#reviews-tab']\"))\n",
    "\n",
    "                        # Wait for the reviews to load\n",
    "                        time.sleep(3)\n",
    "\n",
    "                        # Get the page source and pass it into BeautifulSoup\n",
    "                        # This is requred. I could not get Selenium to understand the HTML in the way bs4 can. Especially with it being mixed HTML and JSON\n",
    "                        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "                        jsonld_script = soup.find('script', {'id': 'bv-jsonld-reviews-data'}).string\n",
    "                        jsonld_data = json.loads(jsonld_script)\n",
    "\n",
    "                        # Extract the review body text\n",
    "                        review_body = jsonld_data['review'][0]['reviewBody']\n",
    "\n",
    "                        # Print the review body\n",
    "                        try:\n",
    "                            print(f\"Review: {review_body}\")\n",
    "                        except:\n",
    "                            print(f\"Couldn't Get Reviews\")\n",
    "\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    return prices\n",
    "\n",
    "\n",
    "def compare_prices(word):\n",
    "    website1_url = 'https://www.livingspaces.com/'\n",
    "    website2_url = 'https://www.rcwilley.com/'\n",
    "\n",
    "    prices_website1 = scrape_website(website1_url, word)\n",
    "    prices_website2 = scrape_website(website2_url, word)\n",
    "\n",
    "search_word = input(\"Enter word to search: \")\n",
    "compare_prices(search_word)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These two websites use dynamically loaded reviews that requires the use of both selenium and bs4 to properly gather.\n",
    "\n",
    "### You must visit each product, then click on the review link and wait for the website to laod in its new HTML. Then we pass that to beautiful soup to decipher the text in the reviews."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
